{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "hw2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ZD-P9wpFGu"
      },
      "source": [
        "# Домашнее задание 2. Классификация, детекция."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaciLcSApFGu"
      },
      "source": [
        "Оценка за часть 1 и часть 2 в этом дз -- по 5 баллов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfw3_Bitp5bk"
      },
      "source": [
        "Выполнила **Александра Романенко** (ИАД-4) и, надеюсь, в этот раз загрузила нужный файл с первого раза ¯\\_(ツ)_/¯\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDiNB6kxpFGu"
      },
      "source": [
        "## Часть 1. Классификация\n",
        "\n",
        "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в датасете есть. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
        "\n",
        "Структура датасета простая -- есть директории train и val, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат собственно сами изображения.\n",
        " \n",
        "__Задание__. Добейтесь accuracy **не менее 0.44**. Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи/блогпосты/вопросы на stackoverflow/видосы от (индийских) ютуберов/курсы/подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
        "\n",
        "В коде ниже необходимо, чтобы код проходил все `assert`'ы.\n",
        "\n",
        "Необходимо написать функцию `predict` по шаблону ниже. Эта функция принимает на вход модель, даталоадер с валидационнами данными, criterion для подсчёта лосса и device, на котором будут производиться вычисления (определён ниже) и возвращает список лоссов по всем объектам, список из предсказанных классов для каждого объекта из из даталоалера и список из настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
        "\n",
        "__Использовать внешние данные для обучения строго запрещено__. Можно использовать предобученные модели из `torchvision`.\n",
        "\n",
        "__Критерии оценки__: Оценка вычисляется по простой формуле: min(5, 5 * Ваша accuracy / 0.44). Оценка округляется до десятых по арифметическим правилам.\n",
        "\n",
        "__Советы и указания__:\n",
        " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать на защите :)\n",
        " - Используйте аугментации. Для этого пользуйтесь модулем torchvision.transforms или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
        " - (ещё раз) Можно файнтюнить предобученные модели из `torchvision`.\n",
        " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом ImageFolder), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
        " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
        " - Чтобы отладить код, пробуйте обучаться на маленькой части датасета. Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
        " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
        " - Фиксируйте random seed.\n",
        " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
        " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
        " - Советуем использовать гпу. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
        " \n",
        "Good luck & have fun! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i1RXULd2Mpa"
      },
      "source": [
        "## ------------------------------------------------------------------------------------------\n",
        "Я долго пыталась с яндексом, но легче оказалось закинуть на дропбокс:)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esxiZK7QuKGz",
        "outputId": "7d021255-ef53-4a8f-9ebf-92762efffcad"
      },
      "source": [
        "! wget https://www.dropbox.com/s/pyfe9tqzjkvvaxu/dataset.zip?dl=0\n",
        "! unzip -q dataset.zip?dl=0"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-25 22:07:46--  https://www.dropbox.com/s/pyfe9tqzjkvvaxu/dataset.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/pyfe9tqzjkvvaxu/dataset.zip [following]\n",
            "--2020-11-25 22:07:46--  https://www.dropbox.com/s/raw/pyfe9tqzjkvvaxu/dataset.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc532023adddb5991c384fc31141.dl.dropboxusercontent.com/cd/0/inline/BD5XHPb_2DS7u1X2WXND62ij37So1XTxIcACzTYJt-xvFhgkFdGPYSl7e68MI5F2HyTUbVNgguc-t6b5l1dTMuJYA9qewZy7tyKWgaRzdDLhUJRQMcUfqleh8fyCBfihZrk/file# [following]\n",
            "--2020-11-25 22:07:46--  https://uc532023adddb5991c384fc31141.dl.dropboxusercontent.com/cd/0/inline/BD5XHPb_2DS7u1X2WXND62ij37So1XTxIcACzTYJt-xvFhgkFdGPYSl7e68MI5F2HyTUbVNgguc-t6b5l1dTMuJYA9qewZy7tyKWgaRzdDLhUJRQMcUfqleh8fyCBfihZrk/file\n",
            "Resolving uc532023adddb5991c384fc31141.dl.dropboxusercontent.com (uc532023adddb5991c384fc31141.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc532023adddb5991c384fc31141.dl.dropboxusercontent.com (uc532023adddb5991c384fc31141.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BD5gqxS2Ku9onMiIB8VhqU6MgXJMrIUv0Km7234b0Y-wUd8tuZHR7U779wdK9yzHzxHYIRHMpT9DO8L6Fgf1ARrV0FuJ-5hjqErAr8ebg1NagyIFFWQdxs4v72f6c46kCeiUxga1zuoYaIHjEAk2O4EKFb9Cx4ymGiutIs54ZTtQC4hPecPs9KdGlxyQQxa9LXa09KGjcWSJKDT-phzV98nWGpHrXrA7g-qCIQZMCtJNc-GeTJQhrdGWPKlVsaK-fn5_YtMYo9zDJY0s1ab-m_uBDrUnq6HiOKAih4zojZd8tbuPkEtQjpmE-2MIgZK-MbfbBqskSiWMa-uPPSMiu4dJFMPU1093oHxuuStSZ07f_w/file [following]\n",
            "--2020-11-25 22:07:47--  https://uc532023adddb5991c384fc31141.dl.dropboxusercontent.com/cd/0/inline2/BD5gqxS2Ku9onMiIB8VhqU6MgXJMrIUv0Km7234b0Y-wUd8tuZHR7U779wdK9yzHzxHYIRHMpT9DO8L6Fgf1ARrV0FuJ-5hjqErAr8ebg1NagyIFFWQdxs4v72f6c46kCeiUxga1zuoYaIHjEAk2O4EKFb9Cx4ymGiutIs54ZTtQC4hPecPs9KdGlxyQQxa9LXa09KGjcWSJKDT-phzV98nWGpHrXrA7g-qCIQZMCtJNc-GeTJQhrdGWPKlVsaK-fn5_YtMYo9zDJY0s1ab-m_uBDrUnq6HiOKAih4zojZd8tbuPkEtQjpmE-2MIgZK-MbfbBqskSiWMa-uPPSMiu4dJFMPU1093oHxuuStSZ07f_w/file\n",
            "Reusing existing connection to uc532023adddb5991c384fc31141.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 268733640 (256M) [application/zip]\n",
            "Saving to: ‘dataset.zip?dl=0.3’\n",
            "\n",
            "dataset.zip?dl=0.3  100%[===================>] 256.28M  35.7MB/s    in 7.5s    \n",
            "\n",
            "2020-11-25 22:07:55 (34.1 MB/s) - ‘dataset.zip?dl=0.3’ saved [268733640/268733640]\n",
            "\n",
            "replace __MACOSX/._dataset? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "y\n",
            "All\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIW5uDSy2FxU"
      },
      "source": [
        "Проверим, что в папочке все нормик:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9zSklu1xgB",
        "outputId": "f88cabf2-4f9e-471d-f659-84f1991cb7c6"
      },
      "source": [
        "!ls ./dataset/dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609q6AG2fy5"
      },
      "source": [
        "Я решила не писать класс MyDataset, а воспользоваться имэджфолдером, как на семинаре"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fki4k4oqpFGv"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "# You may add any imports you need\n",
        "\n",
        "import random\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "import torchvision.models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize, Normalize, ToTensor, Compose\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy2P3wO03lFS"
      },
      "source": [
        "В привычной манере, прём красивый готовый код с семинаров (намбер 5) =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo_vBDWdpFGv"
      },
      "source": [
        "train_transform = Compose([Resize((224, 224)), ToTensor(), Normalize((0.5, 0.5, 0.5), (1, 1, 1)), ])\n",
        "val_transform = Compose([Resize((224, 224)), ToTensor(), Normalize((0.5, 0.5, 0.5), (1, 1, 1)), ])\n",
        "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
        "\n",
        "train_dataset = ImageFolder(\"./dataset/dataset/train\", transform=train_transform)\n",
        "val_dataset = ImageFolder(\"./dataset/dataset/val\", transform=val_transform)\n",
        "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
        "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset -- И Я ТАКИ ТАК И СДЕЛАЮ!)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjGvfTM_pFGv",
        "outputId": "5b1338aa-e764-4a9d-dadf-e036cdb4b5ec"
      },
      "source": [
        "# Just very simple checks\n",
        "assert isinstance(train_dataset[0], tuple)\n",
        "assert len(train_dataset[0]) == 2\n",
        "assert isinstance(train_dataset[1][1], int)\n",
        "print(\"tests passed\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEYR8VER3tpt"
      },
      "source": [
        "УРАААААААААА!!!!! тестс пассд!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcJvLaFNpFGv"
      },
      "source": [
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\", return_accuracy=False):\n",
        "    #model.train()\n",
        "    model = model.to(device).train() # немного меняю исходную, переношу на куду\n",
        "    # готовим почву для всех метрик, которые будут считаться\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    all_losses = []\n",
        "    total_predictions = np.array([])\n",
        "    total_labels = np.array([])\n",
        "    for images, labels in train_dataloader:\n",
        "        # Переносим батч на ГПУ\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Получаем результаты модели\n",
        "        predicted = model(images)\n",
        "        # Узнаем и запоминаем лосс\n",
        "        loss = criterion(predicted, labels)\n",
        "        # Update weights and optimizer operations\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Узнаем и запечатляем все метрики в тоталах\n",
        "        total_loss += loss.item()\n",
        "        total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
        "        total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
        "        num_batches += 1\n",
        "        all_losses.append(loss.detach().item())\n",
        "    \n",
        "    # Подсчет и вывод аккураси\n",
        "    accuracy_train = accuracy_score(total_labels, total_predictions) # беру аккураси скор, потому что дальше в ассертах она и это и правда удобней)\n",
        "    print(\"Accuracy_train:\", round(accuracy_train * 100, 4))\n",
        "    if return_accuracy:\n",
        "      return accuracy_train, all_losses\n",
        "    else:\n",
        "      return all_losses\n",
        "\n",
        "\n",
        "def predict(model, val_dataloader, criterion, device=\"cuda:0\"):\n",
        "    #model.eval()\n",
        "    model.to(device).eval() # немного меняю исходную, переношу на куду\n",
        "    # готовим почву для всех метрик, которые будут считаться\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    total_predictions = np.array([])\n",
        "    total_labels = np.array([])\n",
        "    # Не буду писать комментарии дальше, все аналогично прошлой функции\n",
        "    for images, labels in val_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predicted = model(images)\n",
        "        loss = criterion(predicted, labels)\n",
        "        \n",
        "        accuracy = (predicted.argmax(1) == labels).float().mean()\n",
        "        total_loss += loss.item()\n",
        "        total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
        "        total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
        "        num_batches += 1\n",
        "    metrics = {'loss': total_loss / num_batches}\n",
        "    accuracy_val = accuracy_score(total_predictions, total_labels)\n",
        "    print(\"Accuracy_val:\", round(accuracy_val * 100, 4) )\n",
        "    #return losses, predicted_classes, true_classes \n",
        "    return total_loss, total_predictions, total_labels # изменяю изначальный ритерн, потому что переменные называла иначе\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=5, scheduler=None):\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
        "        # Train, evaluate, print accuracy, make a step of scheduler or whatever you want... I WANNA DIE\n",
        "        # Эпоха на трейне\n",
        "        print(f\"Train Epoch: {epoch}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion\n",
        "        )\n",
        "        # Лосс на трейне сохранили\n",
        "        train_losses.extend(train_losses)\n",
        "\n",
        "        # Эпоха на вале\n",
        "        print(f\"Validation Epoch: {epoch}\")\n",
        "        with torch.no_grad():\n",
        "            validation_loss, predicted_classes, true_classes = predict(\n",
        "                model=model, \n",
        "                val_dataloader=val_dataloader, \n",
        "                criterion=criterion\n",
        "                )\n",
        "        # Лосс на вале сохранили\n",
        "        eval_losses.append(validation_loss)\n",
        "\n",
        "    return train_losses, eval_losses"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT01eRKKFqCE"
      },
      "source": [
        "Возьмем предобученный резнет, заморозим все параметры и обучим последний линейный слой:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlFWzmh0GURB",
        "outputId": "d098a832-397c-4c87-9ab1-1341d934fae7"
      },
      "source": [
        " torchvision.models"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'torchvision.models' from '/usr/local/lib/python3.6/dist-packages/torchvision/models/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E58COv-_Fo-s"
      },
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "# Загрузить предобученную сеть \n",
        "model = resnet18(pretrained=True)\n",
        "# Заморозим параметры\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# В нашем случае последний слой должен выдать 200 классов\n",
        "model.fc = nn.Linear(512, 200)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zz2akFF8pFGv"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.fc.parameters(), 1e-4)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=150, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=150, shuffle=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#scheduler = # LR SCHEDULE THAT YOU PROBABLY CHOOSE\n",
        "n_epochs = 5\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4MFfDkepFGv"
      },
      "source": [
        "Простой тест на проверку правильности написанного кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2JvXc5HpFGv",
        "outputId": "3816d534-e195-4721-d565-62fc9336ec35"
      },
      "source": [
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "print(\"tests passed\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 0.38\n",
            "tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRAAi7OjHYcP"
      },
      "source": [
        "Снова ура!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0akqhNpFGv"
      },
      "source": [
        "Запустить обучение можно в ячейке ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_Nolr1ZpFGv",
        "outputId": "cef7583f-3096-40b9-a88a-92f4e86c5d42"
      },
      "source": [
        "train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0\n",
            "Accuracy_train: 58.219\n",
            "Validation Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [05:07<20:31, 307.93s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 56.61\n",
            "Train Epoch: 1\n",
            "Accuracy_train: 58.885\n",
            "Validation Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|████      | 2/5 [10:16<15:24, 308.07s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 56.95\n",
            "Train Epoch: 2\n",
            "Accuracy_train: 59.432\n",
            "Validation Epoch: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|██████    | 3/5 [15:25<10:16, 308.32s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 57.18\n",
            "Train Epoch: 3\n",
            "Accuracy_train: 59.955\n",
            "Validation Epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|████████  | 4/5 [20:34<05:08, 308.57s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 57.68\n",
            "Train Epoch: 4\n",
            "Accuracy_train: 60.376\n",
            "Validation Epoch: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 5/5 [25:43<00:00, 308.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 58.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [125.237020611763,\n",
              "  122.37832236289978,\n",
              "  120.22852003574371,\n",
              "  117.68891429901123,\n",
              "  116.5314689874649])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95At7rXXpFGv"
      },
      "source": [
        "После всех экспериментов которые вы проделали, выберите лучшую из своих моделей, запустите функцию evaluate. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOB1_0UCpFGv",
        "outputId": "a0d78b09-5059-431f-f6c2-a032bd2b3ad7"
      },
      "source": [
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5*accuracy / 0.44)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy_val: 58.2\n",
            "Оценка за это задание составит 5 баллов\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6l9h0LqpFGw"
      },
      "source": [
        "__Ваш отчёт о проделанных экспериментах__: сначала я писала модель сама, но оказалось, что качество выходит слишком низкое для задания, возможно, требовались серьезные усложнения, но не вышло. Если брать предобученный резнет, как в семинаре, и обучать его полностью, то качество оказывается хуже, кроме того, модель быстрее переобучается. Лучше всего зашла штука с предобученным резнетом18 и обучением последнего слоя, так модель дает наилучшее качество за 4-6 эпох, потом может начинать переобучаться. Очень, конечно, неожиданно, что семинарский вариант оказался настолько крут, но очень приятно:)\n",
        "\n",
        "Полученная accuracy вроде покрывает бонусное задание тоже - так было когда-то, но увы!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKlkN-qBpFGw"
      },
      "source": [
        "## Часть 2. Object detection.\n",
        "\n",
        "В этом задании потребуется обучить детектор фруктов на изображении. Датасет можно скачать [отсюда](https://yadi.sk/d/UPwQB7OZrB48qQ)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec1PPobRpFGw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.pytorch.transforms import ToTensor"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fN2f25SZj6J",
        "outputId": "82d7f6e5-83c0-4e7c-ab6d-831ecfba1036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# ! pip install --upgrade albumentations # может понадобиться"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/b2/9492c74a5d260bc39f0cba9fcdc6652d0f87d342aaeb32197c62029f82df/albumentations-0.5.1-py3-none-any.whl (71kB)\n",
            "\r\u001b[K     |████▋                           | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 30kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 11.3MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "\u001b[K     |████████████████████████████████| 36.7MB 81kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Installing collected packages: imgaug, opencv-python-headless, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.5.1 imgaug-0.4.0 opencv-python-headless-4.4.0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "albumentations",
                  "cv2",
                  "imgaug"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9osR9FGpFGw",
        "outputId": "5d870772-30ca-4f24-83e4-8f5284d1d300"
      },
      "source": [
        "# we will need this library to process the labeling\n",
        "! pip install xmltodict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (0.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od604sOLpFGw"
      },
      "source": [
        "import xmltodict, json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBod8rW1pFGw"
      },
      "source": [
        "Датасет мы за вас написали."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L43ybMbpFGw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xmltodict\n",
        "import json\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "# add any imports you need\n",
        "\n",
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        self.transform = transform\n",
        "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
        "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
        "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
        "            with open(annotation) as f:\n",
        "                annotation_dict = xmltodict.parse(f.read())\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
        "            if not isinstance(objects, list):\n",
        "                objects = [objects]\n",
        "            for obj in objects:\n",
        "                bndbox = obj[\"bndbox\"]\n",
        "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
        "                bbox = list(map(int, bbox))\n",
        "                bboxes.append(torch.tensor(bbox))\n",
        "                labels.append(class2tag[obj[\"name\"]])\n",
        "            self.annotations.append(\n",
        "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.transform:\n",
        "            # the following code is correct if you use albumentations\n",
        "            # if you use torchvision transforms you have to modify it =)\n",
        "            res = self.transform(\n",
        "                image=self.images[i],\n",
        "                bboxes=self.annotations[i][\"boxes\"],\n",
        "                labels=self.annotations[i][\"labels\"],\n",
        "            )\n",
        "            return res[\"image\"], {\n",
        "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
        "                \"labels\": torch.tensor(res[\"labels\"]),\n",
        "            }\n",
        "        else:\n",
        "            return self.images[i], self.annotations[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJGI5nTSpFGw"
      },
      "source": [
        "Выпишем кое-какую техническую работу, которая уже была на семинаре."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7eufXV8pFGw"
      },
      "source": [
        "def intersection_over_union(dt_bbox, gt_bbox):\n",
        "    \"\"\"\n",
        "    Intersection over Union between two bboxes\n",
        "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
        "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
        "    :return : intersection over union\n",
        "    \"\"\"\n",
        "\n",
        "    ## TODO YOUR CODE\n",
        "\n",
        "    intersection_bbox = np.array(\n",
        "        [\n",
        "            max(dt_bbox[0], gt_bbox[0]),\n",
        "            max(dt_bbox[1], gt_bbox[1]),\n",
        "            min(dt_bbox[2], gt_bbox[2]),\n",
        "            min(dt_bbox[3], gt_bbox[3]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
        "        intersection_bbox[3] - intersection_bbox[1], 0\n",
        "    )\n",
        "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
        "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
        "\n",
        "    union_area = area_dt + area_gt - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou\n",
        "\n",
        "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
        "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
        "    gt_labels = target_true[\"labels\"].numpy()\n",
        "\n",
        "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
        "    dt_labels = target_pred[\"labels\"].numpy()\n",
        "    dt_scores = target_pred[\"scores\"].numpy()\n",
        "\n",
        "    results = []\n",
        "    for detection_id in range(len(dt_labels)):\n",
        "        dt_bbox = dt_bboxes[detection_id, :]\n",
        "        dt_label = dt_labels[detection_id]\n",
        "        dt_score = dt_scores[detection_id]\n",
        "\n",
        "        detection_result_dict = {\"score\": dt_score}\n",
        "\n",
        "        max_IoU = 0\n",
        "        max_gt_id = -1\n",
        "        for gt_id in range(len(gt_labels)):\n",
        "            gt_bbox = gt_bboxes[gt_id, :]\n",
        "            gt_label = gt_labels[gt_id]\n",
        "\n",
        "            if gt_label != dt_label:\n",
        "                continue\n",
        "\n",
        "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
        "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
        "                max_gt_id = gt_id\n",
        "\n",
        "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
        "            detection_result_dict[\"TP\"] = 1\n",
        "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
        "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
        "\n",
        "        else:\n",
        "            detection_result_dict[\"TP\"] = 0\n",
        "\n",
        "        results.append(detection_result_dict)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    results = []\n",
        "    model.eval()\n",
        "    nbr_boxes = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (images, targets_true) in enumerate(test_loader):\n",
        "            images = list(image.to(device).float() for image in images)\n",
        "            targets_pred = model(images)\n",
        "            targets_true = [\n",
        "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
        "            ]\n",
        "            targets_pred = [\n",
        "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
        "            ]\n",
        "\n",
        "            for i in range(len(targets_true)):\n",
        "                target_true = targets_true[i]\n",
        "                target_pred = targets_pred[i]\n",
        "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
        "\n",
        "                results.extend(evaluate_sample(target_pred, target_true))\n",
        "\n",
        "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
        "\n",
        "    acc_TP = np.zeros(len(results))\n",
        "    acc_FP = np.zeros(len(results))\n",
        "    recall = np.zeros(len(results))\n",
        "    precision = np.zeros(len(results))\n",
        "\n",
        "    if results[0][\"TP\"] == 1:\n",
        "        acc_TP[0] = 1\n",
        "    else:\n",
        "        acc_FP[0] = 1\n",
        "\n",
        "    for i in range(1, len(results)):\n",
        "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
        "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
        "\n",
        "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
        "        recall[i] = acc_TP[i] / nbr_boxes\n",
        "\n",
        "    return auc(recall, precision)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEEbZbAvpFGw"
      },
      "source": [
        "Вам мы оставляем творческую часть =)\n",
        "\n",
        "__Задание__. Обучите модель для object detection на __обучающем__ датасете и добейтесь PR-AUC не менее __0.91__ на  __тестовом__.\n",
        "\n",
        " - Создайте модель и оптимайзер\n",
        " - Напишите функцию обучения модели\n",
        " - Используйте аугментации\n",
        " \n",
        "Использовать аугментации для обучения __обязательно__. Они дадут 1 балл из 5. Пользуйтесь модулем torchvision.transforms или библиотекой albumentations (о которой говорилось ранее). Последняя библиотека особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Советуем обратить внимание на следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Обратите внимание, что код, написанный в датасете выше, верен только если вы используете albumentations. Если вы выбрали путь torchvision.transforms, вам потребуется метод `__getitem__` изменить (что-то типа `return self.transform(self.images[i])`; однако в таком случае вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную =))\n",
        "\n",
        "Оставшиеся 4 балла вычисляются по простой формуле: __min(4, 4 * Ваш auc / 0.91)__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhfycBjGE4vZ"
      },
      "source": [
        "______________________________________________________________________________________\n",
        "Я снова тырю семинарский код (и не надо меня за это осуждать!), но теперь он из 7ого семинара))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRNCvkHkpFGw"
      },
      "source": [
        "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    n = 0\n",
        "    global_loss = 0\n",
        "    for images, targets in train_dataloader:\n",
        "        images = list(image.to(device).float() for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Берем результаты модели и считаем лоссы\n",
        "        dict_loss = model(images, targets)\n",
        "        losses = sum(loss for loss in dict_loss.values())\n",
        "\n",
        "        # Уже наскучившие шаги оптимайзера, но как же плохо бывает, если о них забыть...\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Добавляем поинт счетчику и закидываем лоссы в общую переменную\n",
        "        n += 1\n",
        "        global_loss += float(losses.cpu().detach().numpy())\n",
        "\n",
        "        # Для красоты показываем суммарный лосс на каждых 10 батчах\n",
        "        if n % 10 == 0:\n",
        "            print(\"Loss value after {} batches is {}\".format(n, round(global_loss / n, 2)))\n",
        "\n",
        "    return global_loss\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.eval()\n",
        "        a = evaluate(model, val_dataloader, device=device)\n",
        "        print(\"AUC ON TEST: {.4f}\".format(a))\n",
        "        model.train()\n",
        "        train_one_epoch(model, dataloader, optimizer, device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rjVeXWg9qf",
        "outputId": "1aed0619-66c0-4cc2-a84a-7256ce9d96d6"
      },
      "source": [
        "! wget https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/hnrO1O5MGy6YZQ\n",
        "! unzip -q hnrO1O5MGy6YZQ"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-25 22:16:30--  https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/hnrO1O5MGy6YZQ\n",
            "Resolving getfile.dokpub.com (getfile.dokpub.com)... 78.46.92.107\n",
            "Connecting to getfile.dokpub.com (getfile.dokpub.com)|78.46.92.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://downloader.disk.yandex.ru/disk/c90b62e10f28a6a2871e37426936c2d0cd2ca51a1d0976ad44e7ac9b46ad542c/5fbf0db9/_YfpybMoWZm47imviby-3T3_P-JmBOk_RsfF-4bIvGOZYs-MElyFZHK9g3qV9TPfb6BX2FY5KQyjMDM5DD9_ng%3D%3D?uid=0&filename=archive.zip&disposition=attachment&hash=RtsMAi9D19CMJn/tsvNcif3gJhcuBwcvnNCRMVWAfEJI9dy17lPafK7a17bPS0oaq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=449558537&fsize=29737028&hid=d79a0119efd953a38f900e15ed7b0a40&media_type=compressed&tknv=v2 [following]\n",
            "--2020-11-25 22:16:30--  https://downloader.disk.yandex.ru/disk/c90b62e10f28a6a2871e37426936c2d0cd2ca51a1d0976ad44e7ac9b46ad542c/5fbf0db9/_YfpybMoWZm47imviby-3T3_P-JmBOk_RsfF-4bIvGOZYs-MElyFZHK9g3qV9TPfb6BX2FY5KQyjMDM5DD9_ng%3D%3D?uid=0&filename=archive.zip&disposition=attachment&hash=RtsMAi9D19CMJn/tsvNcif3gJhcuBwcvnNCRMVWAfEJI9dy17lPafK7a17bPS0oaq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=449558537&fsize=29737028&hid=d79a0119efd953a38f900e15ed7b0a40&media_type=compressed&tknv=v2\n",
            "Resolving downloader.disk.yandex.ru (downloader.disk.yandex.ru)... 77.88.21.127, 2a02:6b8::2:127\n",
            "Connecting to downloader.disk.yandex.ru (downloader.disk.yandex.ru)|77.88.21.127|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s543sas.storage.yandex.net/rdisk/c90b62e10f28a6a2871e37426936c2d0cd2ca51a1d0976ad44e7ac9b46ad542c/5fbf0db9/_YfpybMoWZm47imviby-3T3_P-JmBOk_RsfF-4bIvGOZYs-MElyFZHK9g3qV9TPfb6BX2FY5KQyjMDM5DD9_ng==?uid=0&filename=archive.zip&disposition=attachment&hash=RtsMAi9D19CMJn/tsvNcif3gJhcuBwcvnNCRMVWAfEJI9dy17lPafK7a17bPS0oaq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=449558537&fsize=29737028&hid=d79a0119efd953a38f900e15ed7b0a40&media_type=compressed&tknv=v2&rtoken=uSB1gmcXhaIj&force_default=no&ycrid=na-f11c25d189637729a32a57695428336c-downloader11h&ts=5b4f8ff242040&s=68364762e9f462f523c561db95055f107382f990e7db3b0aa59e78752755bc6c&pb=U2FsdGVkX18Yh6rH3D8itm57nOG5jbtA_6AeKCUesq58RdTG4IGf6rNqvxcPcfZJXX_k_ZWtjdfOv4Ebk6PqoEzg08WrtqePi7XxhspWMo4 [following]\n",
            "--2020-11-25 22:16:31--  https://s543sas.storage.yandex.net/rdisk/c90b62e10f28a6a2871e37426936c2d0cd2ca51a1d0976ad44e7ac9b46ad542c/5fbf0db9/_YfpybMoWZm47imviby-3T3_P-JmBOk_RsfF-4bIvGOZYs-MElyFZHK9g3qV9TPfb6BX2FY5KQyjMDM5DD9_ng==?uid=0&filename=archive.zip&disposition=attachment&hash=RtsMAi9D19CMJn/tsvNcif3gJhcuBwcvnNCRMVWAfEJI9dy17lPafK7a17bPS0oaq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=449558537&fsize=29737028&hid=d79a0119efd953a38f900e15ed7b0a40&media_type=compressed&tknv=v2&rtoken=uSB1gmcXhaIj&force_default=no&ycrid=na-f11c25d189637729a32a57695428336c-downloader11h&ts=5b4f8ff242040&s=68364762e9f462f523c561db95055f107382f990e7db3b0aa59e78752755bc6c&pb=U2FsdGVkX18Yh6rH3D8itm57nOG5jbtA_6AeKCUesq58RdTG4IGf6rNqvxcPcfZJXX_k_ZWtjdfOv4Ebk6PqoEzg08WrtqePi7XxhspWMo4\n",
            "Resolving s543sas.storage.yandex.net (s543sas.storage.yandex.net)... 37.9.68.4, 2a02:6b8:c02:687:0:41af:f1f8:b078\n",
            "Connecting to s543sas.storage.yandex.net (s543sas.storage.yandex.net)|37.9.68.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29737028 (28M) [application/zip]\n",
            "Saving to: ‘hnrO1O5MGy6YZQ.2’\n",
            "\n",
            "hnrO1O5MGy6YZQ.2    100%[===================>]  28.36M  9.76MB/s    in 2.9s    \n",
            "\n",
            "2020-11-25 22:16:35 (9.76 MB/s) - ‘hnrO1O5MGy6YZQ.2’ saved [29737028/29737028]\n",
            "\n",
            "replace test_zip/test/apple_77.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: Al\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YUWQXk_bTHl",
        "outputId": "c65f828d-598e-4c99-ddaa-0bf891482115",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " archive\t       data\t\t     ddPx-LbASMW1Wg.6\n",
            "'archive.zip?dl=0'     dataset\t\t     ddPx-LbASMW1Wg.7\n",
            "'archive.zip?dl=0.1'  'dataset.zip?dl=0'     ddPx-LbASMW1Wg.8\n",
            "'archive.zip?dl=0.2'  'dataset.zip?dl=0.1'   hnrO1O5MGy6YZQ\n",
            "'archive.zip?dl=0.3'  'dataset.zip?dl=0.2'   hnrO1O5MGy6YZQ.1\n",
            "'archive.zip?dl=0.4'  'dataset.zip?dl=0.3'   hnrO1O5MGy6YZQ.2\n",
            " CARVANA.zip\t       ddPx-LbASMW1Wg\t     __MACOSX\n",
            " ChRTDl-sdXPaHQ        ddPx-LbASMW1Wg.1      sample_data\n",
            " ChRTDl-sdXPaHQ.1      ddPx-LbASMW1Wg.2      test_zip\n",
            " ChRTDl-sdXPaHQ.2      ddPx-LbASMW1Wg.3      train_zip\n",
            " ChRTDl-sdXPaHQ.3      ddPx-LbASMW1Wg.4\n",
            " ChRTDl-sdXPaHQ.4      ddPx-LbASMW1Wg.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt35Xum6Uibe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "dIr2lQUWpFGx",
        "outputId": "803d8640-6aaf-4aad-8c80-8686e4fc8324"
      },
      "source": [
        "# Попытаюсь зафиксить сид, вдруг поможет... один из них взяла с https://discuss.pytorch.org/t/random-seed-initialization/7854\n",
        "torch.random.seed()\n",
        "torch.random.manual_seed(13)\n",
        "torch.cuda.manual_seed_all(13)\n",
        "np.random.seed(13)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# В условиях есть гайд к аугментациям, я им сильно воспользовалась дальше\n",
        "train_transform = A.Compose([\n",
        "                             A.RandomCrop(width=450, height=450), # с парикмахерами обычно так, говоришь им \"мне пару см\", а они тебе RandomCrop\n",
        "                             A.HorizontalFlip(p=0.5), # гуччи флип флап, немного поворочаем эту картинку\n",
        "                             A.RandomBrightnessContrast(p=0.2), # ммм шайн брайт лайк э даймонд, даже если ты всего лишь имэйдж\n",
        "                             ToTensor(),\n",
        "                         ], bbox_params=A.BboxParams(format='pascal_voc', min_area=1024, min_visibility=0.1, label_fields=['labels']))\n",
        "                             # Вы спросите откуда 'pascal_voc'? ну, до этого дошла вся флудилка, наверное, дело в том, что он возвращает без нормализации коррдинаты\n",
        "val_transform = A.Compose([\n",
        "                             A.RandomCrop(width=450, height=450), \n",
        "                             A.HorizontalFlip(p=0.5), \n",
        "                             ToTensor(),\n",
        "                         ], bbox_params=A.BboxParams(format='pascal_voc', min_area=1024, min_visibility=0.1, label_fields=['labels']))\n",
        "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION --- Great Hint!\n",
        "\n",
        "train_dataset = FruitDataset('./train_zip/train/', transform=train_transform)\n",
        "val_dataset = FruitDataset(\"./train_zip/test\", transform=val_transform)\n",
        "\n",
        "# Хинт всей моей домашки, что ж, как скажете, беру всю модель с седьмого семинара))\n",
        "# HINT: USE MATERIALS FROM THE SEMINAR\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "def get_detection_model(num_classes=2):\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_detection_model(4)\n",
        "model.to(device)\n",
        "\n",
        "# Я человек простой, в семинаре хорошо работал этот код, значит беру и жду что и здесь должен сработать...\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# ммм теперь сразу будут кортежи, какая красота \n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Не знаю, стоит ли повторять, что почти все параметры я оставила семинарские, ведь они неплохо работают))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "n_epochs = 5 # Не знаю что вы считаете апроприэйт, но меня (да и семинары) 5 устраивает # SELECT APPROPRIZTE NUMBER OF EPOCHS\n",
        "\n",
        "\n",
        "train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7026594443b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-88230569e96a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC ON TEST: {.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-25d017686bcc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TP\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0macc_TP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_p-UT-hpFGx"
      },
      "source": [
        "__Выведите итоговое качество модели__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqqYBSdppFGx"
      },
      "source": [
        "auc = evaluate(model, val_dataloader, criterion)\n",
        "print(\"Оценка за это задание составит {} баллов\".format(min(4, 4 * auc / 0.91)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ZC8g-8pFGx"
      },
      "source": [
        "Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sfhCc-zpFGx"
      },
      "source": [
        "image, labels = next(iter(train_dataset))\n",
        "pred = model(image.unsqueeze(0).to(device))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQWL_qsspFGx"
      },
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "image = torchvision.transform.ToPILImage()(image)\n",
        "draw = ImageDraw.Draw(image)\n",
        "for box in labels['boxes']:\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
        "    \n",
        "for box in pred['boxes']:\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfPzVqapFGx"
      },
      "source": [
        "## Бонус (10 баллов).\n",
        "\n",
        "__Задание__. В части с классификацией добейтесь accuracy не менее 0.52. Напишите отчёт о проделанных экспериментах.\n",
        "\n",
        "__Критерии оценки__. Оценка за бонусную часть равна 10, если вы преодолели качество 0.52 и 0 в противном случае.\n",
        "\n",
        "__Иных оценок кроме 0 и 10 не предусмотрено__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xVeYp-9pFGx"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRsC8F2HpFGx"
      },
      "source": [
        "## Бонус (0 баллов).\n",
        "\n",
        "__Задание 1__. Скиньте ниже смешную картинку, желательно про машинное обучение. На картинке не должно быть никаких упоминаний лектора, семинаристов и ассистентов этого курса.\n",
        "\n",
        "__Задание 2__. Расскажите, как вам задание? Что понравилось, что не понравилось, что можно улучшить? Мы примем во внимание любой фидбек."
      ]
    }
  ]
}